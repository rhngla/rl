# Overview

 
## Agents, actions, observations, and rewards

The reward hypothesis states that all goals can be described by a maximization of expected cumulative reward.

 - An `agent` selects `actions` to maximize `rewards` 
 - Actions can affect subsequent data
 - In contrast to typical supervised and unsupervised learning, data is not treated as i.i.d at different time steps


![The agent receives observations $O_t$ and a scalar feedback signal (reward) $R_t$ at time step $t$, and then performs action $A_t$. The environment evaluates $A_t$ and produces the next observation $O_{t+1}$ and reward $R_{t+1}$](./assets/00_fig_rl.png){width=50% #fig-rl}



::: {.callout-note appearance="simple" icon=false}
Note that the reward is always a scalar. In case of multiple 
objectives, the objectives are eventually weighted and turned into a scalar value.
:::


::: {.column-margin}
We know from *the first fundamental theorem of calculus* that for $x$ in $[a, b]$:
$$\frac{d}{dx}\left( \int_{a}^{x} f(u)\,du\right)=f(x).$$
:::

## History and State
History at time $t$ is the set of all earlier observations, rewards and actions: $H_t = \left\{ O_i, R_i, A_i \right\} \quad \forall \quad i \in \left[ 0, t \right]$. The next step depends on history.

State is a function of history $S_t = f(H_t)$. Portions of state may be private to the environment. The environment state $S_t^e$ specifies the next observation and reward, and can contain information irrelevant to the agent's goals.

Agent state $S_t^a$ is an internal representation that is accessed by the RL algorithm. This is also a function of history.

## Markov states

::: {.callout-note appearance="simple" icon=false}
## Def: Markov state (a.k.a information state)
$P(S_{t+1}|S_{t}) = P(S_{t+1}|S_{1}, ..., S_{t})$. Thus state $S_{t+1}$ only depends on $S_t$ and is independent of $\{S_{1}, ... S_{t-1}\}$
:::

This implies that $S_t$ captures all history relevant for the next step (i.e. future). State is thus a sufficient statistic for the future. 

::: {.callout-note appearance="simple" icon=false}
## Def: Fully observable environment
$O_t = S_t^e = S_t^a$
:::

The setting of a fully observable environment with markov states is formally referred to as a Markov Decision Process (MDP).