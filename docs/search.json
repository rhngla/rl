[
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Overview",
    "section": "",
    "text": "The reward hypothesis states that all goals can be described by a maximization of expected cumulative reward.\n\nAn agent selects actions to maximize rewards\nActions can affect subsequent data\nIn contrast to typical supervised and unsupervised learning, data is not treated as i.i.d at different time steps\n\n\n\n\nFigure 1.1: The agent receives observations \\(O_t\\) and a scalar feedback signal (reward) \\(R_t\\) at time step \\(t\\), and then performs action \\(A_t\\). The environment evaluates \\(A_t\\) and produces the next observation \\(O_{t+1}\\) and reward \\(R_{t+1}\\)\n\n\n\n\n\n\n\n\nNote that the reward is always a scalar. In case of multiple objectives, the objectives are eventually weighted and turned into a scalar value.\n\n\n\n\n\nWe know from the first fundamental theorem of calculus that for \\(x\\) in \\([a, b]\\):\n\\[\\frac{d}{dx}\\left( \\int_{a}^{x} f(u)\\,du\\right)=f(x).\\]"
  },
  {
    "objectID": "intro.html#history-and-state",
    "href": "intro.html#history-and-state",
    "title": "1  Overview",
    "section": "1.2 History and State",
    "text": "1.2 History and State\nHistory at time \\(t\\) is the set of all earlier observations, rewards and actions: \\(H_t = \\left\\{ O_i, R_i, A_i \\right\\} \\quad \\forall \\quad i \\in \\left[ 0, t \\right]\\). The next step depends on history.\nState is a function of history \\(S_t = f(H_t)\\). Portions of state may be private to the environment. The environment state \\(S_t^e\\) specifies the next observation and reward, and can contain information irrelevant to the agent’s goals.\nAgent state \\(S_t^a\\) is an internal representation that is accessed by the RL algorithm. This is also a function of history."
  },
  {
    "objectID": "intro.html#markov-states",
    "href": "intro.html#markov-states",
    "title": "1  Overview",
    "section": "1.3 Markov states",
    "text": "1.3 Markov states\n\n\n\n\n\n\nDef: Markov state (a.k.a information state)\n\n\n\n\\(P(S_{t+1}|S_{t}) = P(S_{t+1}|S_{1}, ..., S_{t})\\). Thus state \\(S_{t+1}\\) only depends on \\(S_t\\) and is independent of \\(\\{S_{1}, ... S_{t-1}\\}\\)\n\n\nThis implies that \\(S_t\\) captures all history relevant for the next step (i.e. future). State is thus a sufficient statistic for the future.\n\n\n\n\n\n\nDef: Fully observable environment\n\n\n\n\\(O_t = S_t^e = S_t^a\\)\n\n\nThe setting of a fully observable environment with markov states is formally referred to as a Markov Decision Process (MDP)."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "Too early for a summary yet!"
  }
]