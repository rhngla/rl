[
  {
    "objectID": "01_fundamentals.html",
    "href": "01_fundamentals.html",
    "title": "2  Fundamentals",
    "section": "",
    "text": "This section will cover the basic framework used for RL."
  },
  {
    "objectID": "01_fundamentals.html#markov-process",
    "href": "01_fundamentals.html#markov-process",
    "title": "2  Fundamentals",
    "section": "2.1 Markov Process",
    "text": "2.1 Markov Process"
  },
  {
    "objectID": "01_fundamentals.html#markov-reward-process",
    "href": "01_fundamentals.html#markov-reward-process",
    "title": "2  Fundamentals",
    "section": "2.2 Markov Reward Process",
    "text": "2.2 Markov Reward Process"
  },
  {
    "objectID": "01_fundamentals.html#markov-decision-process",
    "href": "01_fundamentals.html#markov-decision-process",
    "title": "2  Fundamentals",
    "section": "2.3 Markov Decision Process",
    "text": "2.3 Markov Decision Process"
  },
  {
    "objectID": "00_intro.html",
    "href": "00_intro.html",
    "title": "1  Overview",
    "section": "",
    "text": "The reward hypothesis states that all goals can be described by a maximization of expected cumulative reward.\n\nAn agent selects actions to maximize rewards\nActions can affect subsequent data\nIn contrast to typical supervised and unsupervised learning, data is not treated as i.i.d at different time steps\n\n\n\n\nFigure 1.1: The agent receives observations \\(O_t\\) and a scalar feedback signal (reward) \\(R_t\\) at time step \\(t\\), and then performs action \\(A_t\\). The environment evaluates \\(A_t\\) and produces the next observation \\(O_{t+1}\\) and reward \\(R_{t+1}\\))\n\n\n\n\nWhether we assign time step \\(t\\) or \\(t-1\\) to observation \\(O\\) and reward \\(R\\) in relation to action \\(A_t\\) is a matter of convention. The convention indicated in caption of Figure 1.1 will be followed throughout.\n\n\n\n\n\n\nNote that the reward is always a scalar. In case of multiple objectives, the objectives are eventually weighted and turned into a scalar value."
  },
  {
    "objectID": "00_intro.html#history-and-state",
    "href": "00_intro.html#history-and-state",
    "title": "1  Overview",
    "section": "1.2 History and State",
    "text": "1.2 History and State\nHistory at time \\(t\\) is the set of all earlier observations, rewards and actions: \\(H_t = \\left\\{ O_i, R_i, A_i \\right\\} \\quad \\forall \\quad i \\in \\left[ 0, t \\right]\\). The next step depends on history.\nState is a function of history \\(S_t = f(H_t)\\). Portions of state may be private to the environment. The environment state \\(S_t^e\\) specifies the next observation and reward, and can contain information irrelevant to the agent’s goals.\nAgent state \\(S_t^a\\) is an internal representation that is accessed by the RL algorithm. This is also a function of history."
  },
  {
    "objectID": "00_intro.html#markov-states",
    "href": "00_intro.html#markov-states",
    "title": "1  Overview",
    "section": "1.3 Markov states",
    "text": "1.3 Markov states\n\n\n\n\n\n\nDef: Markov state (a.k.a information state)\n\n\n\n\\(P(S_{t+1}|S_{t}) = P(S_{t+1}|S_{1}, ..., S_{t})\\). Thus state \\(S_{t+1}\\) only depends on \\(S_t\\) and is independent of \\(\\{S_{1}, ... S_{t-1}\\}\\)\n\n\n\\(S_t\\) captures all history relevant for the next step (i.e. future), and so state is a sufficient statistic for the future.\n\n\n\n\n\n\nDef: Fully observable environment\n\n\n\n\\(O_t = S_t^e = S_t^a\\)\n\n\nThe setting of a fully observable environment with markov states is formally referred to as a Markov Decision Process (MDP)."
  },
  {
    "objectID": "00_intro.html#partially-observable-markov-decision-process",
    "href": "00_intro.html#partially-observable-markov-decision-process",
    "title": "1  Overview",
    "section": "1.4 Partially Observable Markov Decision Process",
    "text": "1.4 Partially Observable Markov Decision Process\nCommonly abbreviated as POMDP. Here the agent must construct it’s own state representation \\(S^a_t\\). The role of this representation is to:\n\ncapture history up to that time step \\(H_t\\)\nmaintain beliefs about the environment, e.g. \\(S^a_t = (P[S_t^e=s^1], ... P[S_t^e=s^n])\\)\n\nOne implementation is using recurrent neural networks, as \\(S_t^a = \\sigma(S_{t-1}^aW_s+O_tW_o)\\) where, \\(W_s\\), \\(W_o\\) are learned network weights, and \\(\\sigma\\) is some nonlinearity."
  },
  {
    "objectID": "00_intro.html#rl-agents",
    "href": "00_intro.html#rl-agents",
    "title": "1  Overview",
    "section": "1.5 RL agents",
    "text": "1.5 RL agents\nAgents consist of one or more of the following:\n\nPolicy: specifies behaviour\nValue function: evaluation of states and/or actions\nModel: representation of the environment\n\n\nPolicy\nThe policy function \\(\\pi\\) maps states to actions. It can be deterministic a = or stochastic \\(\\pi(a|s) = P[A=a|S=s]\\)\n\n\nValue function\nDefined for a policy attached to a given state \\(s\\) at time step \\(t\\):\n\\[v_{\\pi}(s)=\\mathbb{E}_{\\pi}[R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + ... | S_t=s]\\]\n\n\nModel\nThe model predicts the environment. Since the environment (see Figure 1.1) determines the next state (observation) and reward, the model:\n\nmaintains a state transition probability: \\(\\mathcal{P}_{ss'}^a = P[S'=s'|S=s,A=a]\\)\nevaluates reward: \\(\\mathcal{R}_s^a = \\mathbb{E}[R|S=s,A=a]\\)"
  },
  {
    "objectID": "00_intro.html#categorizing-rl-agents-and-problems",
    "href": "00_intro.html#categorizing-rl-agents-and-problems",
    "title": "1  Overview",
    "section": "1.6 Categorizing RL agents and problems",
    "text": "1.6 Categorizing RL agents and problems\n\nAgents\n\nValue based: explicit value function, but implicit policy (derived from value function)\nPolicy based: explicit policy function, but no value function\nActor critic: combination of Value- and Policy-based agents\nModel free: policy and/or value function, but no model of environment\nModel based: policy and/or value function, with model of environment\n\n\n\nProblems\n\nRL: environment is initially completely unknown, agent interacts with environment and updates policy\nPlanning: environment is fully known, agent calculates value and updates policy (e.g. tree search)\nExplore/exploit: exploring involves obtaining information about the environment, exploiting involves using that information to maximize reward. Here the agent should discover a good policy through interaction with the environment while minimizing loss of reward.\n\nThe main steps in these problems involve\n\nPrediction: evaluate future, given a policy\nControl: find best policy, optimize for the future"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reinforcement Learning",
    "section": "",
    "text": "These are my notes based on Silver (2015) and Sutton and Barto (2018).\n\n\n\n\nSilver, David. 2015. “Lectures on Reinforcement Learning.” url: https://www.davidsilver.uk/teaching/.\n\n\nSutton, Richard S, and Andrew G Barto. 2018. Reinforcement Learning: An Introduction. MIT press."
  }
]