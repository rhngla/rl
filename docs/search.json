[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reinforcement Learning",
    "section": "",
    "text": "These are my notes based on Silver (2015) and Sutton and Barto (2018).\n\n\n\n\nSilver, David. 2015. “Lectures on Reinforcement Learning.” url: https://www.davidsilver.uk/teaching/.\n\n\nSutton, Richard S, and Andrew G Barto. 2018. Reinforcement Learning: An Introduction. MIT press."
  },
  {
    "objectID": "00_intro.html",
    "href": "00_intro.html",
    "title": "1  Overview",
    "section": "",
    "text": "The reward hypothesis states that all goals can be described by a maximization of expected cumulative reward.\n\nAn agent selects actions to maximize rewards\nActions can affect subsequent data\nIn contrast to typical supervised and unsupervised learning, data is not treated as i.i.d at different time steps\n\n\n\n\nFigure 1.1: The agent receives observations \\(O_t\\) and a scalar feedback signal (reward) \\(R_t\\) at time step \\(t\\), and then performs action \\(A_t\\). The environment evaluates \\(A_t\\) and produces the next observation \\(O_{t+1}\\) and reward \\(R_{t+1}\\))\n\n\n\n\nWhether we assign time step \\(t\\) or \\(t-1\\) to observation \\(O\\) and reward \\(R\\) in relation to action \\(A_t\\) is a matter of convention. The convention indicated in caption of Figure 1.1 will be followed throughout.\n\n\n\n\n\n\nNote that the reward is always a scalar. In case of multiple objectives, the objectives are eventually weighted and turned into a scalar value."
  },
  {
    "objectID": "00_intro.html#history-and-state",
    "href": "00_intro.html#history-and-state",
    "title": "1  Overview",
    "section": "1.2 History and State",
    "text": "1.2 History and State\nHistory at time \\(t\\) is the set of all earlier observations, rewards and actions: \\(H_t = \\left\\{ O_i, R_i, A_i \\right\\} \\quad \\forall \\quad i \\in \\left[ 0, t \\right]\\). The next step depends on history.\nState is a function of history \\(S_t = f(H_t)\\). Portions of state may be private to the environment. The environment state \\(S_t^e\\) specifies the next observation and reward, and can contain information irrelevant to the agent’s goals.\nAgent state \\(S_t^a\\) is an internal representation that is accessed by the RL algorithm. This is also a function of history."
  },
  {
    "objectID": "00_intro.html#markov-state",
    "href": "00_intro.html#markov-state",
    "title": "1  Overview",
    "section": "1.3 Markov state",
    "text": "1.3 Markov state\n\n\n\n\n\n\n\nDefinition 1.1 (Markov state) \\[P(S_{t+1}|S_{t}) = P(S_{t+1}|S_{1}, ..., S_{t})\\]\nThus state \\(S_{t+1}\\) only depends on \\(S_t\\) and is independent of \\(\\{S_{1}, ... S_{t-1}\\}\\). The markov state is also referred to as information state\n\n\n\n\n\\(S_t\\) captures all history relevant for the next step (i.e. future), and so state is a sufficient statistic for the future.\n\n\n\n\n\n\n\nDefinition 1.2 (Fully observable environment) \n\\[O_t = S_t^e = S_t^a\\]\n\n\n\n\nThe setting of a fully observable environment with markov states is formally referred to as a Markov Decision Process (MDP)."
  },
  {
    "objectID": "00_intro.html#partially-observable-markov-decision-process",
    "href": "00_intro.html#partially-observable-markov-decision-process",
    "title": "1  Overview",
    "section": "1.4 Partially Observable Markov Decision Process",
    "text": "1.4 Partially Observable Markov Decision Process\nCommonly abbreviated as POMDP. Here the agent must construct it’s own state representation \\(S^a_t\\). The role of this representation is to:\n\ncapture history up to that time step \\(H_t\\)\nmaintain beliefs about the environment, e.g. \\(S^a_t = (P[S_t^e=s^1], ... P[S_t^e=s^n])\\)\n\nOne implementation is using recurrent neural networks, as \\(S_t^a = \\sigma(S_{t-1}^aW_s+O_tW_o)\\) where, \\(W_s\\), \\(W_o\\) are learned network weights, and \\(\\sigma\\) is some nonlinearity."
  },
  {
    "objectID": "00_intro.html#rl-agents",
    "href": "00_intro.html#rl-agents",
    "title": "1  Overview",
    "section": "1.5 RL agents",
    "text": "1.5 RL agents\nAgents consist of one or more of the following:\n\nPolicy: specifies behaviour\nValue function: evaluation of states and/or actions\nModel: representation of the environment\n\n\nPolicy\nThe policy function \\(\\pi\\) maps states to actions. It can be deterministic \\(a = \\pi(s)\\) or stochastic \\(\\pi(a|s) = P[A=a|S=s]\\).\n\n\nValue function\nDefined for a policy attached to a given state \\(s\\) at time step \\(t\\):\n\\[v_{\\pi}(s)=\\mathbb{E}_{\\pi}[R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + ... | S_t=s]\\]\n\n\nModel\nThe model predicts the environment. Since the environment (see Figure 1.1) determines the next state (observation) and reward, the model:\n\nmaintains a state transition probability: \\(\\mathcal{P}_{ss'}^a = P[S'=s'|S=s,A=a]\\)\nevaluates reward: \\(\\mathcal{R}_s^a = \\mathbb{E}[R|S=s,A=a]\\)"
  },
  {
    "objectID": "00_intro.html#categorizing-rl-agents-and-problems",
    "href": "00_intro.html#categorizing-rl-agents-and-problems",
    "title": "1  Overview",
    "section": "1.6 Categorizing RL agents and problems",
    "text": "1.6 Categorizing RL agents and problems\n\nAgents\n\nValue based: explicit value function, but implicit policy (derived from value function)\nPolicy based: explicit policy function, but no value function\nActor critic: combination of Value- and Policy-based agents\nModel free: policy and/or value function, but no model of environment\nModel based: policy and/or value function, with model of environment\n\n\n\nSee OpenAI (2020) for a taxonomy of problems in RL, and an implementation focussed approach to RL.\n\n\nProblems\n\nRL: environment is initially completely unknown, agent interacts with environment and updates policy\nPlanning: environment is fully known, agent calculates value and updates policy (e.g. tree search)\nExplore/exploit: exploring involves obtaining information about the environment, exploiting involves using that information to maximize reward. Here the agent should discover a good policy through interaction with the environment while minimizing loss of reward.\n\nThe main steps in these problems involve\n\nPrediction: evaluate future, given a policy\nControl: find best policy, optimize for the future\n\n\n\n\n\nOpenAI. 2020. “Spinning up in Deep RL.” url: https://spinningup.openai.com/."
  },
  {
    "objectID": "01_fundamentals.html",
    "href": "01_fundamentals.html",
    "title": "2  Fundamentals",
    "section": "",
    "text": "This section will cover the basic framework used for RL.\nA Markov Decision Process (MDP) formally describes the environment for RL, where the environment is fully observable. Partially observable environments can be converted to a Markov Decision Process. In particular, bandits are a Markov Decision Process with one state."
  },
  {
    "objectID": "01_fundamentals.html#markov-process",
    "href": "01_fundamentals.html#markov-process",
    "title": "2  Fundamentals",
    "section": "2.1 Markov Process",
    "text": "2.1 Markov Process\nFrom Definition 1.1, a state is \\(S_t\\) is Markov if\n\\[P(S_{t+1}|S_{t}) = P(S_{t+1}|S_{1}, ..., S_{t}) \\tag{2.1}\\]\nFor a Markov state \\(s\\) and next state \\(s'\\), the state transition probability is\n\\[\\mathcal{P_{ss'}} = P\\left[S_{t+1}=s'|S_{t}=s' \\right] \\tag{2.2}\\]\nThe state transition probabilities can be defined as a matrix, with rows sums \\(\\sum_j \\mathcal{P_ij} = 1\\) up to 1 as follows:\n\\[\n\\mathcal{P} = \\begin{bmatrix}\n              \\mathcal{P}_{11} & \\dots & \\mathcal{P}_{1n} \\\\\n               \\vdots & \\ddots & \\vdots \\\\\n              \\mathcal{P}_{n1} & \\dots & \\mathcal{P}_{nn}\n              \\end{bmatrix}\n\\tag{2.3}\\]\nA Markov process is equivalent to a Markov chain. It is a memoryless random process, and consists of a sequence of random states \\(S_{1}, S_{2}, \\dots\\) with the Markov property.\n\n\n\n\n\n\n\nDefinition 2.1 (Markov chain) A tuple \\((S, \\mathcal{P})\\) defines a Markov chain, where\n\n\\(S\\) is a finite set of states\n\\(\\mathcal{P}\\) is the state transition probability matrix\n\n\n\n\n\nSpecific instances of state-to-state transitions are drawn from a Markov chain.\nFor a stationary Markov process, \\(\\mathcal{P}\\) is time invariant. In the context of a Reinforcement Learning problem, non-stationary Markov processes can be accommodated by\n\nincrementally adjusting solutions\nintroducing additional states and converting to a stationary Markov process."
  },
  {
    "objectID": "01_fundamentals.html#markov-reward-process",
    "href": "01_fundamentals.html#markov-reward-process",
    "title": "2  Fundamentals",
    "section": "2.2 Markov Reward Process",
    "text": "2.2 Markov Reward Process\n\n\n\n\n\n\n\nDefinition 2.2 (Markov Reward Process) A tuple \\((\\mathcal{S}, \\mathcal{P}, {\\color{Green}R, \\gamma})\\) defines a Markov reward process, where\n\n\\(S\\) is a finite set of states\n\\(\\mathcal{P}\\) is the state transition probability matrix\n\\({\\color{Green}R}\\) is a rewards function\n\\({\\color{Green}\\gamma}\\) is a discount factor \\(\\in [0,1]\\)\n\n\n\n\n\nNote that we mostly care about the reward associated with state \\(s\\), \\(\\mathcal{r_s} = \\mathbb{E}[R_{t+1} | S_t=s]\\).\n\n\n\n\n\n\n\nDefinition 2.3 (Return/Goal) \n\\[\nG_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2R_{t+3} + ... = \\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}\n\\]\n\n\n\n\nNote that \\(G_t\\) is a random variable, and we mostly care about \\(\\mathbb{E}[{G_t}]\\).\nAccording to this formulation, the discount factor determines the present value of future rewards. \\(\\gamma \\rightarrow 0\\) weighs the immediate reward highly, whereas \\(\\gamma \\rightarrow 1\\) accounts for future rewards almost as much as the immediate reward.\n\n\nIn practice, the choice of \\(\\gamma\\) in range \\([0,1)\\) is required for convergence reasons, although it conveniently reflects a recency bias and accounts for uncertainty - both in the future and in the internal model of the environment.\n\n\n\n\n\n\n\nDefinition 2.4 (Value function) The value function \\(v(s)\\) is defined as the expected return starting from state \\(s\\):\n\\[\nv(s) = \\mathbb{E}[{G_t} | {S_t = s}]\n\\]\n\n\n\n\n\n2.2.1 Bellman equation for a Markov Rewards Process:\nThe value function is broken down into the immediate reward \\(R_{t+1}\\) and a discounted value-of-successor function \\(\\gamma v(S_{t+1})\\).\n\\[\nv(s) = \\mathbb{E}[R_{t+1} + \\gamma v(S_{t+1}) | {S_t = s}]\n\\]\n\\[\n\\therefore v(s) = r_s + \\gamma \\sum_{s' \\in S} {\\mathcal{P}_{ss'} v({s'})}\n\\tag{2.4}\\]\nWe can represent the Bellman equation using matrices:\n\\[\n\\mathcal{V} = \\mathcal{R} + \\gamma \\mathcal{P}\\mathcal{V}\n\\tag{2.5}\\]\n\\[\n\\begin{bmatrix}\nv({s_{1}}) \\\\ \\vdots \\\\ v({s_{n}})\n\\end{bmatrix}  =  \\begin{bmatrix}\nr({s_{1}}) \\\\ \\vdots \\\\ r({s_{n}})\n\\end{bmatrix} + \\gamma\n\\begin{bmatrix}\n\\mathcal{P}_{11} & \\dots & \\mathcal{P}_{1n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\mathcal{P}_{n1} & \\dots & \\mathcal{P}_{nn}\n\\end{bmatrix}\n\\begin{bmatrix}\nv({s_{1}}) \\\\ \\vdots \\\\ v({s_{n}})\n\\end{bmatrix}\n\\]\nUsing Equation 2.5 we see that \\(\\mathcal{V} = (I - \\gamma \\mathcal{P})^{-1}\\mathcal{R}\\)\nThe computational complexity to obtain this solution is \\(\\mathcal{O}(n^3)\\) for \\(n\\) states. Such solutions can thus be calculated for a small number of states. Iterative methods are used for cases where the number of states is large. Examples of such methods are:\n\nDynamic programming\nMote carlo evaluation\nTemporal difference learning"
  },
  {
    "objectID": "01_fundamentals.html#markov-decision-process",
    "href": "01_fundamentals.html#markov-decision-process",
    "title": "2  Fundamentals",
    "section": "2.3 Markov Decision Process",
    "text": "2.3 Markov Decision Process"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "OpenAI. 2020. “Spinning up in Deep RL.” url: https://spinningup.openai.com/.\n\n\nSilver, David. 2015. “Lectures on Reinforcement Learning.”\nurl: https://www.davidsilver.uk/teaching/.\n\n\nSutton, Richard S, and Andrew G Barto. 2018. Reinforcement Learning:\nAn Introduction. MIT press."
  }
]