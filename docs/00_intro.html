<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.0.37">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Reinforcement Learning - 1&nbsp; Overview</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./01_fundamentals.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Overview</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Reinforcement Learning</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00_intro.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Overview</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_fundamentals.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Fundamentals</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#agents-actions-observations-and-rewards" id="toc-agents-actions-observations-and-rewards" class="nav-link active" data-scroll-target="#agents-actions-observations-and-rewards"> <span class="header-section-number">1.1</span> Agents, actions, observations, and rewards</a></li>
  <li><a href="#history-and-state" id="toc-history-and-state" class="nav-link" data-scroll-target="#history-and-state"> <span class="header-section-number">1.2</span> History and State</a></li>
  <li><a href="#markov-state" id="toc-markov-state" class="nav-link" data-scroll-target="#markov-state"> <span class="header-section-number">1.3</span> Markov state</a></li>
  <li><a href="#partially-observable-markov-decision-process" id="toc-partially-observable-markov-decision-process" class="nav-link" data-scroll-target="#partially-observable-markov-decision-process"> <span class="header-section-number">1.4</span> Partially Observable Markov Decision Process</a></li>
  <li><a href="#rl-agents" id="toc-rl-agents" class="nav-link" data-scroll-target="#rl-agents"> <span class="header-section-number">1.5</span> RL agents</a>
  <ul class="collapse">
  <li><a href="#policy" id="toc-policy" class="nav-link" data-scroll-target="#policy">Policy</a></li>
  <li><a href="#value-function" id="toc-value-function" class="nav-link" data-scroll-target="#value-function">Value function</a></li>
  <li><a href="#model" id="toc-model" class="nav-link" data-scroll-target="#model">Model</a></li>
  </ul></li>
  <li><a href="#categorizing-rl-agents-and-problems" id="toc-categorizing-rl-agents-and-problems" class="nav-link" data-scroll-target="#categorizing-rl-agents-and-problems"> <span class="header-section-number">1.6</span> Categorizing RL agents and problems</a>
  <ul class="collapse">
  <li><a href="#agents" id="toc-agents" class="nav-link" data-scroll-target="#agents">Agents</a></li>
  <li><a href="#problems" id="toc-problems" class="nav-link" data-scroll-target="#problems">Problems</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Overview</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<section id="agents-actions-observations-and-rewards" class="level2 page-columns page-full" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="agents-actions-observations-and-rewards"><span class="header-section-number">1.1</span> Agents, actions, observations, and rewards</h2>
<p>The reward hypothesis states that all goals can be described by a maximization of expected cumulative reward.</p>
<ul>
<li>An <code>agent</code> selects <code>actions</code> to maximize <code>rewards</code></li>
<li>Actions can affect subsequent data</li>
<li>In contrast to typical supervised and unsupervised learning, data is not treated as i.i.d at different time steps</li>
</ul>
<div id="fig-rl" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./assets/00_fig_rl.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Figure 1.1: The agent receives observations <span class="math inline">\(O_t\)</span> and a scalar feedback signal (reward) <span class="math inline">\(R_t\)</span> at time step <span class="math inline">\(t\)</span>, and then performs action <span class="math inline">\(A_t\)</span>. The environment evaluates <span class="math inline">\(A_t\)</span> and produces the next observation <span class="math inline">\(O_{t+1}\)</span> and reward <span class="math inline">\(R_{t+1}\)</span>)</figcaption><p></p>
</figure>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p>Whether we assign time step <span class="math inline">\(t\)</span> or <span class="math inline">\(t-1\)</span> to observation <span class="math inline">\(O\)</span> and reward <span class="math inline">\(R\)</span> in relation to action <span class="math inline">\(A_t\)</span> is a matter of convention. The convention indicated in caption of <a href="#fig-rl">Figure&nbsp;<span>1.1</span></a> will be followed throughout.</p>
</div></div><div id="note-scalar-reward" class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div id="note-scalar-reward" class="callout-body-container">
<p>Note that the reward is always a scalar. In case of multiple objectives, the objectives are eventually weighted and turned into a scalar value.</p>
</div>
</div>
</div>
</section>
<section id="history-and-state" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="history-and-state"><span class="header-section-number">1.2</span> History and State</h2>
<p>History at time <span class="math inline">\(t\)</span> is the set of all earlier observations, rewards and actions: <span class="math inline">\(H_t = \left\{ O_i, R_i, A_i \right\} \quad \forall \quad i \in \left[ 0, t \right]\)</span>. The next step depends on history.</p>
<p>State is a function of history <span class="math inline">\(S_t = f(H_t)\)</span>. Portions of state may be private to the environment. The environment state <span class="math inline">\(S_t^e\)</span> specifies the next observation and reward, and can contain information irrelevant to the agent’s goals.</p>
<p>Agent state <span class="math inline">\(S_t^a\)</span> is an internal representation that is accessed by the RL algorithm. This is also a function of history.</p>
</section>
<section id="markov-state" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="markov-state"><span class="header-section-number">1.3</span> Markov state</h2>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-markov-state" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.1 (Markov state) </strong></span><span class="math display">\[P(S_{t+1}|S_{t}) = P(S_{t+1}|S_{1}, ..., S_{t})\]</span></p>
<p>Thus state <span class="math inline">\(S_{t+1}\)</span> only depends on <span class="math inline">\(S_t\)</span> and is independent of <span class="math inline">\(\{S_{1}, ... S_{t-1}\}\)</span>. The markov state is also referred to as <em>information state</em></p>
</div>
</div>
</div>
</div>
<p><span class="math inline">\(S_t\)</span> captures all history relevant for the next step (i.e.&nbsp;future), and so state is a sufficient statistic for the future.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-fully-observable-environment" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.2 (Fully observable environment) </strong></span></p>
<p><span class="math display">\[O_t = S_t^e = S_t^a\]</span></p>
</div>
</div>
</div>
</div>
<p>The setting of a fully observable environment with markov states is formally referred to as a Markov Decision Process (MDP).</p>
</section>
<section id="partially-observable-markov-decision-process" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="partially-observable-markov-decision-process"><span class="header-section-number">1.4</span> Partially Observable Markov Decision Process</h2>
<p>Commonly abbreviated as POMDP. Here the agent must construct it’s own state representation <span class="math inline">\(S^a_t\)</span>. The role of this representation is to:</p>
<ul>
<li>capture history up to that time step <span class="math inline">\(H_t\)</span></li>
<li>maintain beliefs about the environment, e.g.&nbsp;<span class="math inline">\(S^a_t = (P[S_t^e=s^1], ... P[S_t^e=s^n])\)</span></li>
</ul>
<p>One implementation is using recurrent neural networks, as <span class="math inline">\(S_t^a = \sigma(S_{t-1}^aW_s+O_tW_o)\)</span> where, <span class="math inline">\(W_s\)</span>, <span class="math inline">\(W_o\)</span> are learned network weights, and <span class="math inline">\(\sigma\)</span> is some nonlinearity.</p>
</section>
<section id="rl-agents" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="rl-agents"><span class="header-section-number">1.5</span> RL agents</h2>
<p><code>Agents</code> consist of one or more of the following:</p>
<ol type="1">
<li><code>Policy</code>: specifies behaviour</li>
<li><code>Value function</code>: evaluation of states and/or actions</li>
<li><code>Model</code>: representation of the environment</li>
</ol>
<section id="policy" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="policy">Policy</h3>
<p>The policy function <span class="math inline">\(\pi\)</span> maps states to actions. It can be deterministic a = or stochastic <span class="math inline">\(\pi(a|s) = P[A=a|S=s]\)</span></p>
</section>
<section id="value-function" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="value-function">Value function</h3>
<p>Defined for a policy attached to a given state <span class="math inline">\(s\)</span> at time step <span class="math inline">\(t\)</span>:</p>
<p><span class="math display">\[v_{\pi}(s)=\mathbb{E}_{\pi}[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... | S_t=s]\]</span></p>
</section>
<section id="model" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="model">Model</h3>
<p>The model predicts the environment. Since the environment (see <a href="#fig-rl">Figure&nbsp;<span>1.1</span></a>) determines the next state (observation) and reward, the model:</p>
<ul>
<li>maintains a state transition probability: <span class="math inline">\(\mathcal{P}_{ss'}^a = P[S'=s'|S=s,A=a]\)</span></li>
<li>evaluates reward: <span class="math inline">\(\mathcal{R}_s^a = \mathbb{E}[R|S=s,A=a]\)</span></li>
</ul>
</section>
</section>
<section id="categorizing-rl-agents-and-problems" class="level2 page-columns page-full" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="categorizing-rl-agents-and-problems"><span class="header-section-number">1.6</span> Categorizing RL agents and problems</h2>
<section id="agents" class="level3 unnumbered page-columns page-full">
<h3 class="unnumbered anchored" data-anchor-id="agents">Agents</h3>
<ul>
<li><strong>Value based</strong>: explicit value function, but implicit policy (derived from value function)</li>
<li><strong>Policy based</strong>: explicit policy function, but no value function</li>
<li><strong>Actor critic</strong>: combination of Value- and Policy-based agents</li>
<li><strong>Model free</strong>: policy and/or value function, but no model of environment</li>
<li><strong>Model based</strong>: policy and/or value function, with model of environment</li>
</ul>

<div class="no-row-height column-margin column-container"><div class="">
<p>See <span class="citation" data-cites="openai2020">OpenAI (<a href="references.html#ref-openai2020" role="doc-biblioref">2020</a>)</span> for a taxonomy of problems in RL, and an implementation focussed approach to RL.</p>
</div></div></section>
<section id="problems" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="problems">Problems</h3>
<ul>
<li><strong>RL</strong>: environment is initially completely unknown, agent interacts with environment and updates policy</li>
<li><strong>Planning</strong>: environment is fully known, agent calculates value and updates policy (e.g.&nbsp;tree search)</li>
<li><strong>Explore/exploit</strong>: exploring involves obtaining information about the environment, exploiting involves using that information to maximize reward. Here the agent should discover a good policy through interaction with the environment while minimizing loss of reward.</li>
</ul>
<p>The main steps in these problems involve</p>
<ol type="1">
<li><strong>Prediction</strong>: evaluate future, given a policy</li>
<li><strong>Control</strong>: find best policy, optimize for the future</li>
</ol>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-openai2020" class="csl-entry" role="doc-biblioentry">
OpenAI. 2020. <span>“Spinning up in Deep RL.”</span> <span class="smallcaps">url:</span>&nbsp;<a href="https://spinningup.openai.com/" class="uri">https://spinningup.openai.com/</a>.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Preface</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./01_fundamentals.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Fundamentals</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>